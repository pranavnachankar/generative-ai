{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35949560-145d-4dc4-af0d-cf24378c15e8",
   "metadata": {},
   "source": [
    "## Getting started with Langchain and Google Gen AI\n",
    "- Setup of LangChain, LangSmith and LangServe\n",
    "- Use the basic components of LangChain : prompt templates, models, output parsers\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b9b997-2df5-4069-8fc7-a5ffe4ecfea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# genAI API key\n",
    "GENERATIVE_AI_API_KEY = os.getenv(\"GENERATIVE_AI_API_KEY\")\n",
    "\n",
    "# LangChain Tracing :\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = \"true\"\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983779d3-f6fc-417d-9a9e-8fa3ab15d611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-1.5-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f3b48cb9b10> async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7f3b48cbafb0> default_metadata=()\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GENERATIVE_AI_API_KEY) # must needs to pass the `model`\n",
    "# supported Google_GenAI models :\n",
    "# https://pub.dev/documentation/langchain_google/latest/langchain_google/ChatGoogleGenerativeAI-class.html\n",
    "# https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# https://ai.google.dev/gemini-api/docs/models/experimental-models\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e0670d-4786-49b8-a6b1-3dbde02194c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"## Generative AI with LangChain: A Powerful Combination\\n\\n**Generative AI** focuses on creating new content, such as text, images, audio, or even code, based on existing data. Think of it as a creative machine learning model that can generate novel outputs. \\n\\n**LangChain** is a framework that helps you build and deploy applications powered by LLMs (Large Language Models) like ChatGPT or Bard. It acts as a bridge between your application and the powerful capabilities of these models.\\n\\n**Combining Generative AI with LangChain empowers you to:**\\n\\n* **Harness the power of LLMs for real-world tasks:**  LangChain provides tools to interact with LLMs, allowing you to use them for tasks like summarization, translation, question answering, and creative writing.\\n* **Connect LLMs to external data sources:**  LangChain enables you to integrate LLMs with databases, APIs, and other data sources, allowing them to access and process real-world information.\\n* **Build complex and sophisticated applications:**  LangChain offers building blocks for creating complex workflows, combining different LLMs, data sources, and other tools to achieve specific goals.\\n* **Make LLMs more useful and practical:**  LangChain helps address limitations of LLMs, such as their reliance on context and their inability to access external data, making them more applicable to real-world scenarios.\\n\\n**Here's a simple analogy:**\\n\\nImagine a chef (LLM) with incredible culinary skills but limited access to ingredients. LangChain acts as a personal assistant who can source the necessary ingredients (data), prepare them (process and structure data), and deliver them to the chef so they can create amazing dishes (generate output).\\n\\n**Examples of Generative AI applications powered by LangChain:**\\n\\n* **Chatbots:** Create engaging and informative chatbots that can access external knowledge sources to answer user queries.\\n* **Document summarization and analysis:**  Summarize large documents, extract key insights, and generate reports based on the information.\\n* **Content creation:**  Generate creative text formats like poems, scripts, or marketing copy.\\n* **Code generation:**  Use LLMs to generate code snippets or even complete programs.\\n\\n**Benefits of using Generative AI with LangChain:**\\n\\n* **Increased productivity:** Automate repetitive tasks and generate content efficiently.\\n* **Enhanced creativity:** Explore new ideas and generate novel outputs.\\n* **Improved user experience:** Create more engaging and personalized interactions.\\n* **Unlock new possibilities:**  Build innovative applications that leverage the power of LLMs.\\n\\n**In conclusion, LangChain empowers developers to build powerful and practical applications by seamlessly integrating generative AI models into their workflows. This combination unlocks a world of possibilities, allowing you to leverage the power of LLMs for a wide range of real-world tasks.** \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-4822facf-937d-4909-93c0-3af2e9e11b71-0', usage_metadata={'input_tokens': 10, 'output_tokens': 576, 'total_tokens': 586})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input and get response from llm\n",
    "llm.invoke(\"What is Generative AI with Langchain ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25670726-98f6-43f0-a2d7-953493592a82",
   "metadata": {},
   "source": [
    "### Chat prompt Template :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79508ad-10a6-4d9f-b961-b057002c981a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question.\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633ca63b-7e3b-4a15-848e-5bb30fe01a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangSmith is a platform designed to streamline and improve the development and management of large language models (LLMs). It offers a suite of tools and features that help developers build, test, deploy, and monitor their LLMs more effectively.\\n\\n**Key Features of LangSmith:**\\n\\n* **Experiment Tracking:** Track and manage experiments with different LLM configurations, training data, and hyperparameters. This allows for easy comparison and analysis of model performance.\\n* **Version Control:** Version control for LLMs, enabling you to track changes, revert to previous versions, and collaborate with other developers.\\n* **Model Deployment:**  Deploy trained LLMs to production environments with ease, ensuring efficient and scalable model serving.\\n* **Model Monitoring:** Monitor the performance of deployed LLMs over time, identifying potential issues and ensuring continued accuracy.\\n* **Data Management:**  Organize and manage training data, including data labeling, annotation, and version control.\\n* **Collaboration Tools:**  Collaborate with other developers on LLM projects, sharing code, experiments, and results.\\n* **Integration with Popular Frameworks:** LangSmith integrates with popular frameworks like Hugging Face, TensorFlow, and PyTorch.\\n\\n**Benefits of Using LangSmith:**\\n\\n* **Increased Efficiency:** Streamlines the LLM development process, saving time and effort.\\n* **Improved Reproducibility:** Ensures that experiments can be easily reproduced, fostering trust in results.\\n* **Enhanced Collaboration:** Facilitates collaboration among developers, leading to faster progress.\\n* **Better Model Performance:** Provides tools for optimizing LLM performance and identifying areas for improvement.\\n* **Reduced Risk:** Helps mitigate risks associated with deploying LLMs, such as data drift and model degradation.\\n\\n**Who Should Use LangSmith?**\\n\\nLangSmith is suitable for a wide range of users involved in LLM development, including:\\n\\n* Data Scientists\\n* Machine Learning Engineers\\n* Researchers\\n* Software Developers\\n* Anyone working with LLMs in various industries.\\n\\n**Conclusion:**\\n\\nLangSmith is a powerful platform that simplifies the development and management of LLMs, enabling developers to build, test, deploy, and monitor their models more effectively. Its comprehensive features and intuitive interface make it a valuable tool for anyone working with LLMs.\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-c7b43db1-6d45-451a-8a19-19de1b643850-0' usage_metadata={'input_tokens': 24, 'output_tokens': 457, 'total_tokens': 481}\n"
     ]
    }
   ],
   "source": [
    "chain = promt|llm\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a95b8-fd90-4fa7-bca8-90328307c0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
